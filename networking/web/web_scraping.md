
libraries
1. requests lib to get page
2. scrapy to crawl set of urls
3. beautifulsoup to parse
4. lxml to parse
5. frontera

difficulties
1. page format changes
2. dynamic content - use headless chrome + phantomjs
3. honeypot traps - color blends with background color, hidden tags, set display none
4. captcha

guidelines
1. respect robots.txt
2. rate limit
3. user agent spoofing
4. rotate ip and proxy
5. do not have same crawling pattern
6. off-peak hours
7. use canonical urls

https://velotio.com/blog/2018/2/28/web-scraping-introduction-best-practices-caveats
