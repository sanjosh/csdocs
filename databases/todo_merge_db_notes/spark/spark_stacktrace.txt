/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -javaagent:/home/rana/idea-IC-171.4073.35/lib/idea_rt.jar=38705:/home/rana/idea-IC-171.4073.35/bin -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/icedtea-sound.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/management-agent.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/rt.jar:/home/rana/tmp/vish_Rana/spark-ganesha-connector/target/scala-2.11/classes:/home/rana/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/home/rana/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-unsafe_2.11/jars/spark-unsafe_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-tags_2.11/jars/spark-tags_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-sql_2.11/jars/spark-sql_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-sketch_2.11/jars/spark-sketch_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-network-shuffle_2.11/jars/spark-network-shuffle_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-network-common_2.11/jars/spark-network-common_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-launcher_2.11/jars/spark-launcher_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-core_2.11/jars/spark-core_2.11-2.1.1.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-catalyst_2.11/jars/spark-catalyst_2.11-2.1.1.jar:/home/rana/tmp/vish_Rana/spark-ganesha-connector/lib/jetcd-0.1.0-SNAPSHOT.jar:/home/rana/.ivy2/cache/io.netty/netty-transport/jars/netty-transport-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-resolver/jars/netty-resolver-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-handler-proxy/jars/netty-handler-proxy-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-handler/jars/netty-handler-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-common/jars/netty-common-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-codec-socks/jars/netty-codec-socks-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-codec-http2/jars/netty-codec-http2-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-codec-http/jars/netty-codec-http-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-codec/jars/netty-codec-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.netty/netty-buffer/jars/netty-buffer-4.1.8.Final.jar:/home/rana/.ivy2/cache/io.grpc/grpc-stub/jars/grpc-stub-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-protobuf-nano/jars/grpc-protobuf-nano-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-protobuf-lite/jars/grpc-protobuf-lite-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-protobuf/jars/grpc-protobuf-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-okhttp/jars/grpc-okhttp-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-netty/jars/grpc-netty-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-core/jars/grpc-core-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-context/jars/grpc-context-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-auth/jars/grpc-auth-1.3.0.jar:/home/rana/.ivy2/cache/io.grpc/grpc-all/jars/grpc-all-1.3.0.jar:/home/rana/.ivy2/cache/com.squareup.okio/okio/jars/okio-1.6.0.jar:/home/rana/.ivy2/cache/com.squareup.okhttp/okhttp/jars/okhttp-2.5.0.jar:/home/rana/.ivy2/cache/com.google.protobuf.nano/protobuf-javanano/bundles/protobuf-javanano-3.0.0-alpha-5.jar:/home/rana/.ivy2/cache/com.google.protobuf/protobuf-lite/bundles/protobuf-lite-3.0.1.jar:/home/rana/.ivy2/cache/com.google.protobuf/protobuf-java-util/bundles/protobuf-java-util-3.2.0.jar:/home/rana/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-3.3.1.jar:/home/rana/.ivy2/cache/com.google.instrumentation/instrumentation-api/jars/instrumentation-api-0.3.0.jar:/home/rana/.ivy2/cache/com.google.errorprone/error_prone_annotations/jars/error_prone_annotations-2.0.19.jar:/home/rana/.ivy2/cache/com.google.code.gson/gson/jars/gson-2.7.jar:/home/rana/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-3.0.0.jar:/home/rana/.ivy2/cache/com.google.auth/google-auth-library-credentials/jars/google-auth-library-credentials-0.4.0.jar:/home/rana/.ivy2/cache/com.google.api.grpc/grpc-google-common-protos/jars/grpc-google-common-protos-0.1.6.jar:/home/rana/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/home/rana/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/home/rana/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.6.jar:/home/rana/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/home/rana/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/home/rana/.ivy2/cache/org.sonatype.sisu.inject/cglib/jars/cglib-2.2.1-v20090111.jar:/home/rana/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/home/rana/.ivy2/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.21.jar:/home/rana/.ivy2/cache/org.slf4j/jul-to-slf4j/jars/jul-to-slf4j-1.7.16.jar:/home/rana/.ivy2/cache/org.slf4j/jcl-over-slf4j/jars/jcl-over-slf4j-1.7.16.jar:/home/rana/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.1.jar:/home/rana/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.0.jar:/home/rana/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/home/rana/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.10.jar:/home/rana/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.0.jar:/home/rana/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/home/rana/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/home/rana/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/home/rana/.ivy2/cache/org.json4s/json4s-native_2.11/jars/json4s-native_2.11-3.2.10.jar:/home/rana/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/home/rana/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/home/rana/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/home/rana/.ivy2/cache/org.javassist/javassist/bundles/javassist-3.18.1-GA.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/home/rana/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/home/rana/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/home/rana/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/home/rana/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/home/rana/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/home/rana/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/home/rana/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/home/rana/.ivy2/cache/org.codehaus.janino/janino/jars/janino-3.0.0.jar:/home/rana/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-3.0.0.jar:/home/rana/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/home/rana/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/home/rana/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.5.jar:/home/rana/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/home/rana/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.3.jar:/home/rana/.ivy2/cache/org.apache.spark/spark-sql-kafka-0-10_2.11/jars/spark-sql-kafka-0-10_2.11-2.1.0.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.8.1.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.8.1.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.8.1.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.8.1.jar:/home/rana/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.8.1.jar:/home/rana/.ivy2/cache/org.apache.kafka/kafka-clients/jars/kafka-clients-0.10.0.1.jar:/home/rana/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/home/rana/.ivy2/cache/org.apache.httpcomponents/httpcore/jars/httpcore-4.4.1.jar:/home/rana/.ivy2/cache/org.apache.httpcomponents/httpclient/jars/httpclient-4.4.1.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.2.0.jar:/home/rana/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.4.0.jar:/home/rana/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.4.0.jar:/home/rana/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.4.0.jar:/home/rana/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/home/rana/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/home/rana/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.5.jar:/home/rana/.ivy2/cache/org.apache.commons/commons-crypto/jars/commons-crypto-1.0.0.jar:/home/rana/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/home/rana/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/home/rana/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/home/rana/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/home/rana/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/home/rana/.ivy2/cache/org.antlr/antlr4-runtime/jars/antlr4-runtime-4.5.3.jar:/home/rana/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.4.jar:/home/rana/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.13.jar:/home/rana/.ivy2/cache/net.jpountz.lz4/lz4/jars/lz4-1.3.0.jar:/home/rana/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.1.jar:/home/rana/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/home/rana/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/home/rana/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/home/rana/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/home/rana/.ivy2/cache/javax.inject/javax.inject/jars/javax.inject-1.jar:/home/rana/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/home/rana/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.0.42.Final.jar:/home/rana/.ivy2/cache/io.netty/netty/bundles/netty-3.8.0.Final.jar:/home/rana/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.2.jar:/home/rana/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.2.jar:/home/rana/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.2.jar:/home/rana/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.2.jar:/home/rana/.ivy2/cache/commons-net/commons-net/jars/commons-net-2.2.jar:/home/rana/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.2.jar:/home/rana/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.5.jar:/home/rana/.ivy2/cache/commons-io/commons-io/jars/commons-io-2.1.jar:/home/rana/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/home/rana/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/home/rana/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/home/rana/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:/home/rana/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.10.jar:/home/rana/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/home/rana/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/home/rana/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/home/rana/.ivy2/cache/com.univocity/univocity-parsers/jars/univocity-parsers-2.2.1.jar:/home/rana/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.0.jar:/home/rana/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.0.jar:/home/rana/.ivy2/cache/com.thoughtworks.paranamer/paranamer/jars/paranamer-2.6.jar:/home/rana/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/home/rana/.ivy2/cache/com.google.inject/guice/jars/guice-3.0.jar:/home/rana/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.5.jar:/home/rana/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.6.5.jar:/home/rana/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.6.5.jar:/home/rana/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.6.5.jar:/home/rana/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.6.5.jar:/home/rana/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/home/rana/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/home/rana/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/home/rana/.ivy2/cache/com.google.guava/guava/bundles/guava-21.0.jar examples.GaneshaReadDF
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/07/06 15:56:13 INFO SparkContext: Running Spark version 2.1.1
17/07/06 15:56:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/06 15:56:13 INFO SecurityManager: Changing view acls to: rana
17/07/06 15:56:13 INFO SecurityManager: Changing modify acls to: rana
17/07/06 15:56:13 INFO SecurityManager: Changing view acls groups to: 
17/07/06 15:56:13 INFO SecurityManager: Changing modify acls groups to: 
17/07/06 15:56:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(rana); groups with view permissions: Set(); users  with modify permissions: Set(rana); groups with modify permissions: Set()
17/07/06 15:56:14 INFO Utils: Successfully started service 'sparkDriver' on port 34099.
17/07/06 15:56:14 INFO SparkEnv: Registering MapOutputTracker
17/07/06 15:56:14 INFO SparkEnv: Registering BlockManagerMaster
17/07/06 15:56:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/07/06 15:56:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/07/06 15:56:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3c24d3a-c13b-44aa-b52d-65508343e1c4
17/07/06 15:56:14 INFO MemoryStore: MemoryStore started with capacity 1940.7 MB
17/07/06 15:56:14 INFO SparkEnv: Registering OutputCommitCoordinator
17/07/06 15:56:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/07/06 15:56:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.1.6.180:4040
17/07/06 15:56:14 INFO Executor: Starting executor ID driver on host localhost
17/07/06 15:56:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37535.
17/07/06 15:56:14 INFO NettyBlockTransferService: Server created on 10.1.6.180:37535
17/07/06 15:56:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/07/06 15:56:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.1.6.180, 37535, None)
17/07/06 15:56:14 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.6.180:37535 with 1940.7 MB RAM, BlockManagerId(driver, 10.1.6.180, 37535, None)
17/07/06 15:56:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.1.6.180, 37535, None)
17/07/06 15:56:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.1.6.180, 37535, None)
17/07/06 15:56:14 INFO SharedState: Warehouse path is 'file:/home/rana/tmp/vish_Rana/spark-ganesha-connector/spark-warehouse/'.
root
 |-- ID: integer (nullable = false)
 |-- Name: string (nullable = false)
 |-- Age: short (nullable = true)
 |-- Salary: long (nullable = true)
 |-- Expenses: long (nullable = true)

17/07/06 15:56:17 INFO CodeGenerator: Code generated in 265.28351 ms
17/07/06 15:56:17 INFO SparkContext: Starting job: foreachPartition at GaneshaReadDF.scala:24
17/07/06 15:56:17 INFO DAGScheduler: Got job 0 (foreachPartition at GaneshaReadDF.scala:24) with 2 output partitions
17/07/06 15:56:17 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at GaneshaReadDF.scala:24)
17/07/06 15:56:17 INFO DAGScheduler: Parents of final stage: List()
17/07/06 15:56:17 INFO DAGScheduler: Missing parents: List()
17/07/06 15:56:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at foreachPartition at GaneshaReadDF.scala:24), which has no missing parents
17/07/06 15:56:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.3 KB, free 1940.7 MB)
17/07/06 15:56:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1940.7 MB)
17/07/06 15:56:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.6.180:37535 (size: 5.4 KB, free: 1940.7 MB)
17/07/06 15:56:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/07/06 15:56:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at foreachPartition at GaneshaReadDF.scala:24)
17/07/06 15:56:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
17/07/06 15:56:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6284 bytes)
17/07/06 15:56:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6166 bytes)
17/07/06 15:56:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/07/06 15:56:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/07/06 15:56:17 INFO CodeGenerator: Code generated in 28.84564 ms
iter Count: 0
17/07/06 15:56:18 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1231 bytes result sent to driver
17/07/06 15:56:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 150 ms on localhost (executor driver) (1/2)
17/07/06 15:56:18 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: "ABC"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.castTo(GaneshaRelation.scala:158)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.dataMapToRow(GaneshaRelation.scala:139)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.createRow(GaneshaRelation.scala:123)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.convertToRow(GaneshaRelation.scala:116)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.size(TraversableOnce.scala:107)
	at scala.collection.AbstractIterator.size(Iterator.scala:1334)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:25)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:24)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/06 15:56:18 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NumberFormatException: For input string: "ABC"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.castTo(GaneshaRelation.scala:158)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.dataMapToRow(GaneshaRelation.scala:139)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.createRow(GaneshaRelation.scala:123)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.convertToRow(GaneshaRelation.scala:116)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.size(TraversableOnce.scala:107)
	at scala.collection.AbstractIterator.size(Iterator.scala:1334)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:25)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:24)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/07/06 15:56:18 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
17/07/06 15:56:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/07/06 15:56:18 INFO TaskSchedulerImpl: Cancelling stage 0
17/07/06 15:56:18 INFO DAGScheduler: ResultStage 0 (foreachPartition at GaneshaReadDF.scala:24) failed in 0.320 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NumberFormatException: For input string: "ABC"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.castTo(GaneshaRelation.scala:158)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.dataMapToRow(GaneshaRelation.scala:139)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.createRow(GaneshaRelation.scala:123)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.convertToRow(GaneshaRelation.scala:116)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.size(TraversableOnce.scala:107)
	at scala.collection.AbstractIterator.size(Iterator.scala:1334)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:25)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:24)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
17/07/06 15:56:18 INFO DAGScheduler: Job 0 failed: foreachPartition at GaneshaReadDF.scala:24, took 0.540625 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NumberFormatException: For input string: "ABC"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.castTo(GaneshaRelation.scala:158)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.dataMapToRow(GaneshaRelation.scala:139)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.createRow(GaneshaRelation.scala:123)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.convertToRow(GaneshaRelation.scala:116)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.size(TraversableOnce.scala:107)
	at scala.collection.AbstractIterator.size(Iterator.scala:1334)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:25)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:24)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at examples.GaneshaReadDF$.delayedEndpoint$examples$GaneshaReadDF$1(GaneshaReadDF.scala:24)
	at examples.GaneshaReadDF$delayedInit$body.apply(GaneshaReadDF.scala:9)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.App$$anonfun$main$1.apply(App.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.App$class.main(App.scala:76)
	at examples.GaneshaReadDF$.main(GaneshaReadDF.scala:9)
	at examples.GaneshaReadDF.main(GaneshaReadDF.scala)
Caused by: java.lang.NumberFormatException: For input string: "ABC"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.castTo(GaneshaRelation.scala:158)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.dataMapToRow(GaneshaRelation.scala:139)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.createRow(GaneshaRelation.scala:123)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$.convertToRow(GaneshaRelation.scala:116)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at io.dcengines.ganesha.spark.connector.sql.datasource.ganesha.GaneshaRelation$$anonfun$buildScan$1.apply(GaneshaRelation.scala:109)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.size(TraversableOnce.scala:107)
	at scala.collection.AbstractIterator.size(Iterator.scala:1334)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:25)
	at examples.GaneshaReadDF$$anonfun$1.apply(GaneshaReadDF.scala:24)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/06 15:56:18 INFO SparkContext: Invoking stop() from shutdown hook
17/07/06 15:56:18 INFO SparkUI: Stopped Spark web UI at http://10.1.6.180:4040
17/07/06 15:56:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.1.6.180:37535 in memory (size: 5.4 KB, free: 1940.7 MB)
17/07/06 15:56:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/07/06 15:56:18 INFO MemoryStore: MemoryStore cleared
17/07/06 15:56:18 INFO BlockManager: BlockManager stopped
17/07/06 15:56:18 INFO BlockManagerMaster: BlockManagerMaster stopped
17/07/06 15:56:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/07/06 15:56:18 INFO SparkContext: Successfully stopped SparkContext
17/07/06 15:56:18 INFO ShutdownHookManager: Shutdown hook called
17/07/06 15:56:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-45f78b94-b9bb-4ae5-aa24-09d46de1601e

Process finished with exit code 1

