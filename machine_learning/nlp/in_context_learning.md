
# An Explanation of In-context Learning as Implicit Bayesian Inference 

 In-context learning is a mysterious emergent behavior in large language models (LMs) where the LM performs a task just by conditioning on input-output examples, without optimizing any parameters.
 
 Latent concepts

1. effect of pretraining data (long term coherence)
2. Model scale
3. Architecture (e.g., decoder-only vs. encoder-decoder)
4. objective (e.g., casual LM vs. masked LM) 
 
 # Ref
 
 1. http://ai.stanford.edu/blog/understanding-incontext/
 
 
