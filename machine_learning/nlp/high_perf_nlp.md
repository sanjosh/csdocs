
EMNLP 2020 tutorial

http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf

# distillation
# quantization
# pruning
# efficient attention

Data independent patterns
1. Blockwise Transformer (Qiu et al., 2019)
1. Sparse Transformer (Child et al., 2019)
1. Longformer (Beltagy et al., 2020)
1. Big Bird (Zaheer et al., 2020)

Data-Dependent Patterns
1. Linformer (Wang et al., 2020)
1. Reformer (Kitaev et al., 2020)
1. Routing Transformer (Roy et al., 2020)
1. Clustered Attention (Vyas et al., 2020)
1. Sinkhorn Transformer (Tay et al., 2020)

Kernels and Alternative Attention Mechanisms
1. Linear Transformer (Katharopoulos et al., 2020)
1. Random Feature Attention (Anonymous, 2020)
1. Performer (Choromanski et al., 2020)
1. Synthesizer (Tay et al., 2020)

Recurrence
1. Transformer XL (Dai et al., 2019)
1. Compressive Transformers (Rae et al., 2019
