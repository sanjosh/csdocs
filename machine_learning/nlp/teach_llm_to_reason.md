

https://dennyzhou.github.io/Teach-LLMs-to-Reason-7-2023.pdf

1. Chain-of-thought prompting: <question, rationale, answer>
2. Self-consistency: solve multiple times and choose the most common answer
3. Least-to-most prompting: decompose to easier subproblems
4. Instruction finetuning: mixing up exemplars to enable zero-sho

Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny
Zhou. Compositional Semantic Parsing with Large Language Models. ICLR 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet,
Quoc Le, Ed Chi. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. ICLR 2023.

Lewkowycz et al., 2022. Solving Quantitative Reasoning Problems With Language Models.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou.
Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023
