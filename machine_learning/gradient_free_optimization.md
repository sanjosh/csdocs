

where explicit expressions of gradient are infeasible, but evaluations of objective func at specific points are available 

# Derivative free optimization

# Zeroth order optimization

advantages

1. convergence rate is comparable to first-order optimization
1. computationally efficient approx to derivatives
1. easy to implement with small modification of commonly used gradient-based algo


# References

https://analyticsindiamag.com/zeroth-order-optimisation-and-its-applications-in-deep-learning/

Sijia Lu, Zeroth order optimization http://www.comp.hkbu.edu.hk/~iib/2018/Dec/article4/iib_vol19no2_article4.pdf
