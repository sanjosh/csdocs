
# Autoencoder

example of Feedforward network. 

Learn imperfectly.  Used for dimensionality reduction or feature lerning. Learn structure of manifold

```
h = f(x); r = g(h)
f is encoder
g is decoder
```

training via
1. minibatch gradient descent
2. recirculation

Minimize loss func L(x, g(f(x)))

## why useful

purpose is not to learn identity func

autoencoder useful because hidden dimension h can take useful properties, when h is constrained to have smaller dim than x

autoencoders fail to learn anything useful 
1. if the encoder and decoder are given too much capacity.
2. if h has same or greater dim as x

## linear and non-linear

When the decoder is linear and L is the mean squared error, an undercomplete
autoencoder learns to span the same subspace as PCA.  Autoencoder learns principal subspace of training data.

Autoencoders with nonlinear encoder functions f and nonlinear decoder func-
tions g can thus learn a more powerful nonlinear generalization of PCA.

## regularized autoencoder

regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output

## sparse autoencoder

Add L1 penalty on latent space into loss function.  This ensures latent space is sparse or minimal in terms of variable size

## denoising autoencoder

Learn L(x, g(f(x'))) where x' is corrupted copy of x

Denoising autoencoders must undo this corruption rather than simply copying their input.  They are an example of how overcomplete, high-capacity models may be used as autoencoders so long as care is taken to prevent them from learning the identity function.

## Variational Autoencoders

Carl Doersch.  tutorial on variational autoencoders

https://arxiv.org/pdf/1606.05908.pdf

VAE tries to model this process: given an image x, we want to find at least one latent vector which is able to describe it; one vector that contains the instructions to generate x. Formulating it using the law of total probability, we get P(x)=∫P(x|z)P(z)dz

The VAE training objective is to maximize P(x) or ELBO

It turns out every distribution can be generated by applying a sufficiently complicated function over a standard multivariate Gaussian.

Hence, we'll choose P(z) to be a standard multivariate Gaussian. f, being modeled by a neural network, can thus be broken to two phases:

The first layers will map the Gaussian to the true distribution over the latent space. We won't be able to interpret the dimensions, but it doesn't really matter.  The later layers will then map from the latent space to P(x|z).

Yoel Zeldes. http://anotherdatum.com/vae.html
http://anotherdatum.com/vae2.html

### Why variational auto-encoder

The fundamental problem with (basic) autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous, or allow easy interpolation.

VAE latent spaces are, by design, continuous, allowing easy random sampling and interpolation.  Its encoder outputs 2 vectors of size n : mean and standard deviation.  This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.  

To allow smooth interpolation in encodings, introduce KL divergence into the loss function.   Minimizing the KL divergence here means optimizing the probability distribution parameters (μ and σ) to closely resemble that of the target distribution.

Intuitively, this loss encourages the encoder to distribute all encodings (for all types of inputs, eg. all MNIST numbers), evenly around the center of the latent space. 

Optimizing the two together, however, results in the generation of a latent space which maintains the similarity of nearby encodings on the local scale via clustering, yet globally, is very densely packed near the latent space origin (compare the axes with the original).


https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf

Kingma, Autoencoding variational bayes https://arxiv.org/pdf/1312.6114.pdf

Kingma, An Introduction to Variational Autoencoders
https://arxiv.org/pdf/1906.02691.pdf

http://www.robots.ox.ac.uk/~sjrob/Pubs/vbTutorialFinal.pdf. Fox Roberts A tutorial on variational Bayesian

Doetsch tutorial variational autoencoders https://arxiv.org/abs/1606.05908

https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf

https://www.jeremyjordan.me/variational-autoencoders/

https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/

https://ijdykeman.github.io/ml/2016/12/21/cvae.html

http://cse.iitkgp.ac.in/~sudeshna/courses/DL17/Autoencoder-15-Mar-17.pdf

https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#td-vae

## Deep encoder and decoder

Depth can exponentially reduce the computational cost of representing some
functions. Depth can also exponentially decrease the amount of training data
needed to learn some functions

## stochastic encoder and decoder

encoding and decoding func are not simple; involve noise injection

## contractive autoencoder

denoising autoencoders make the reconstruction function resist small but finite-sized perturbations of the input, while contractive autoencoders make the feature extraction function resist infinitesimal perturbations of the input.

##  regularized autoencoder

https://github.com/ParthaEth/Regularized_autoencoders-RAE-

# autoencoders - sebastian raschka

https://youtu.be/UnImUYOdWgk

VAE latent space has normal distribution so it's easier to sample

issues Regular auto encoder
1. Distribution not centered
2. Can be odd shaped so hard to sample in balanced fashion
2. May not be continuous

Log VAR trick

Minimize ELBO (evidence lower bound) which is sum of reconstruction loss and KL loss

Minimise KL divergence between latent space and normal distribution ensures smooth latent space

Minimizing reconstruction loss ensures good reconstruction

To generate new images just sample from latent space by running only decoder

Upsampling + regular convolution is better than transpose convolution for images.  See distill pub article

Add smile vector in latent space to an image

## Convolutional autoencoder

Uses transposed convolution in decoder

## Fully connected autoencoder

After training ignore the decoder and use encoder output as embedding


## relation to PCA

PCA is equivalent to autoencoder with linear activation function.

In autoencoder the hidden dimensions are not orthogonal unlike PCA

## Applications

1. Dimension reduction
2. Privacy
2. Noise elimination : by using denoising autoencoder which uses dropout
2. Compression

### Semantic hashing

if we train the dimensionality reduction algorithm to produce a code that is low- dimensional and binary , then we can store all database entries in a hash table mapping binary code vectors to entries.

This hash table allows us to perform information retrieval by returning all database entries that have the same binary code as the query. We can also search over slightly less similar entries very efficiently, just by flipping individual bits from the encoding of the query. This approach to information retrieval via dimensionality reduction and binarization is called semantic hashing

# Reference

1. Goodfellow.  Deep Learning Chap 14


# TODO

https://arxiv.org/abs/2201.03898 
An Introduction to Autoencoders Umberto Michelucci
