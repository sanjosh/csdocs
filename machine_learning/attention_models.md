
types of attention
1. Multi-head 
2. scaled dot product

An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.

1. Additive attention
1. Multiplicative attention (dot product)


https://nlp.seas.harvard.edu/2018/04/03/attention.html

# Reference

1. Yannic Kilcher.  https://www.youtube.com/watch?v=iDulhoQ2pro
2. Vaswani.  Attention is all you need.  https://arxiv.org/abs/1706.03762
