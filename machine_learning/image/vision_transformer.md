
A recent trend in computer vision is to replace convolutions with transformers

```
Transformers densely model relationships between its inputs 
â€“ ideal for late stages of a neural network, when 
concepts are sparse and spatially-distant, but 
extremely inefficient for early stages of a network, when
patterns are redundant and localized.
```

1. convolution = learn local interactions
1. self-attention = learn global interaction

# ViT

# DeiT

# T2T

# vision transformer more robust than CNN

occlusion

distribution shift

adversarial perturbation

natural perturbation

Intriguing Properties of Vision Transformers
https://arxiv.org/pdf/2105.10497.pdf

Visual Transformers: Where Do Transformers Really Belong in Vision Models?
https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Visual_Transformers_Where_Do_Transformers_Really_Belong_in_Vision_Models_ICCV_2021_paper.pdf
