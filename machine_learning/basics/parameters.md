
# batch size 

The batch size limits the number of samples to be shown to the network before a weight update can be performed

network with higher batch size is faster to train

keep it low for CPU-based training; higher for GPU

# number of iterations

Number of iterations * batch size = entire training dataset

# epoch 

one epoch is when entire training dataset is passed ONCE forward and backward through neural network

# loss function

# learning rate

https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network

Leslie Smith paper on 1-cycle learning

# activation

data normalization, activation and loss func are important in determining accuracy

# optimizer

TODO how to embedding

TODO padding

optimizer just speeds up convergence
