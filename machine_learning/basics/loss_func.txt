
# Loss functions

1. Mean squared
2. Cross-Entropy

https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/

"this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes."

https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation

https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/
