
Convolutional layer

-----
 
# Pooling layer

aggregate regions that are spaced "s" neurons apart, where s = stride length. 
mean
max
make network translation-invariant

-----
 
# Output layer

for classification problems, each neuron's value lies in [0, 1]

-------

# Neural net

0. Linear : no hidden layers equiv to linear logistic regression.  assumes input data is linearly separable.
1. Fully connected  : ignore spatial structure
2. CNN : learn local feature
3. Capsule : 


---------

neural net is traind to minimize a cost function
1. for binary classification : cross-entropy error function
3. for multiclass : categorical cross-entropy

--------------

* Input layers : build embedding; feature space

* Activation layer : ReLU, tanh, sigmoid
ReLU is preferred because it simplifies backpropagation, makes learning faster and also avoids saturation.
Alleviate vanishing gradient problem

* Pooling for down-sampling
To control over-fitting
Max Pooling 

* Dropout layers 

* Fully connected 
learn correlation.  Neurons have large receptive field

* Classification
Softmax : Convert to probabilistic score
Hierarchical softmax  :

* Skip Connection

---------

Ref
https://adeshpande3.github.io/
