
# Loss functions

## Mean squared

## Cross-Entropy

for multi-class classification

## triplet loss 

between anchor, positive and negative loss

## CTC Loss 

align two sequences

## reconstruction loss 

auto-encoder

## Hinge loss 

max margin classification.  for learning nonlinear embeddings or semi-supervised learning.

## KL divergence loss 

## generalized end-to-end

## L2

## cycle consistency

GAN


https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/

"this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes."

https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation

https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/

https://pytorch.org/docs/stable/nn.html#loss-functions
