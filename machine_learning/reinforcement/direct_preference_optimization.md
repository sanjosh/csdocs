
https://twitter.com/ericmitchellai/status/1663597641127833600

https://arxiv.org/abs/2305.18290

```
 Like existing
algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that
measures how well a given reward function aligns with empirical preference data. However, while
existing methods use the preference model to define a preference loss to train a reward model and
then train a policy that optimizes the learned reward model, DPO uses a change of variables to define
the preference loss as a function of the policy directly.

our key insight is to leverage an analytical
mapping from reward functions to optimal policies, which enables us to transform a loss function
over reward functions into a loss function over policies

Bradley-Terry model,  more general Plackett-Luce ranking model for human preferences
```
