
https://www.youtube.com/watch?v=Yg3q5x7yDeM
Jay Alammar

https://ex.pegg.io/


Hima Lakkaraju

https://explainml-tutorial.github.io/aaai21

https://explainml-tutorial.github.io/neurips20

https://drive.google.com/file/d/1xn2dCDAeEEhB_rex202KxMPqIPj31fZ4/view

https://www.chilconference.org/tutorial_T04.html

https://interpretable-ml-class.github.io/

# Shapley value

1. Model-agnostic: Use with any model
1. Theoretic foundation: Game theory
1. Good software ecosystem
1. Local and global explanations

https://christophm.github.io/interpretable-ml-book/shapley.html

## tutorial


To understand a feature’s importance in a model
it is necessary to understand both how changing that feature
1. impacts the model’s output
2. the distribution of that feature’s values.

The core idea behind Shapley value based explanations of machine learning models is to use fair allocation results from cooperative game theory to allocate credit for a model’s output among its input features.

In order to connect game theory with machine learning models, it is necessary to both match
1. model’s input features with players in a game,
2. match the model function with the rules of the game

https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html

# Captum

```
Concept activation vectors (CAVs) are a technique to explain a neural network’s 
internal state by associating model predictions with concepts (such as “apron,” “cafe”, etc.) 
that people can easily understand.
```

https://ai.facebook.com/blog/new-captum-version-features-more-ways-to-build-ai-responsibly

# Fiddler platform


# alex smola lecture 15

Shapley value function

https://www.youtube.com/watch?v=IZD_4OVcypI

https://www.youtube.com/watch?v=KMc2k211lKM

https://www.youtube.com/watch?v=_raedQsEGI8

https://www.youtube.com/watch?v=GtAVDsNvU8U
