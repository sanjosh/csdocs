
# Aggregating Nested Transformers

https://www.youtube.com/watch?v=XJE2CY1p0EM

 nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner.

Vision Transformer (ViT)

when trained on smaller datasets, ViT usually underperforms its convnet counterparts.

Lack of inductive bias such as locality and translation equivariance is one explanation for the data inefficiency of ViT models

These type of insights align with the recent methods with local self-attention and hierarchical transformer structures

To promote information communication across patches, they propose specialized design such as “haloing operation” [51] and “shifted window


 https://github.com/google-research/nested-transformer

https://arxiv.org/abs/2105.12723
